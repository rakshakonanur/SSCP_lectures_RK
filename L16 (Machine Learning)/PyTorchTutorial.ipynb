{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a17d1a-adf1-4c6f-a865-8b61a9f8f703",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial\n",
    "\n",
    "Based on: https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "\n",
    "- **Problem:** Fit a third-order polynomial $a + b x + c x^2 + d  x^3$ to the sine function $y = \\sin(x)$.\n",
    "\n",
    "- **Method:** Use gradient descent to train the four parameters $(a, b, c, d)$ by minimizing the Euclidean distance between the predicted and true outputs.\n",
    "\n",
    "- **Warm-up:** First, the problem will be solved using NumPy to manually implement the forward and backward passes.\n",
    "    - NumPy is a general scientific computing library and is not specialized for deep learning or computation graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501448f2-1db6-4319-81ce-2941546e0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, label=\"True model\")\n",
    "ax.set(xlabel=\"x\", ylabel=r\"$y=\\sin(x)$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a00d7b-27bd-491e-8048-2035de37d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x** 2 + d * x** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x**2).sum()\n",
    "    grad_d = (grad_y_pred * x**3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n",
    "\n",
    "# Plot prediction\n",
    "y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, label=\"True model\")\n",
    "ax.plot(x, y_pred, label=\"Learned model\")\n",
    "ax.set(xlabel=\"x\", ylabel=r\"$y=\\sin(x)$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07dd157-9772-4601-a63e-64c9d5323f80",
   "metadata": {},
   "source": [
    "### PyTorch: Tensors\n",
    "\n",
    "\n",
    "- **NumPy's Limitation**: NumPy cannot use GPUs, which makes it too slow for modern deep learning where GPUs offer significant speedups (50x or more).\n",
    "\n",
    "- **PyTorch's Solution: The Tensor:**\n",
    "    - A PyTorch Tensor is conceptually the same as a NumPy array (an n-dimensional array).\n",
    "    - Unlike NumPy, Tensors can run on GPUs for accelerated computation.\n",
    "    - They can also automatically track a computational graph and gradients.\n",
    "\n",
    "- **Next Step:** We will now use PyTorch Tensors to solve the same problem, still requiring the manual implementation of the forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41e681-e463-42d1-9b16-d76634aaa386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n",
    "\n",
    "# Plot prediction\n",
    "y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, label=\"True model\")\n",
    "ax.plot(x, y_pred, label=\"Learned model\")\n",
    "ax.set(xlabel=\"x\", ylabel=r\"$y=\\sin(x)$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe293f3-97b4-4c4c-ab95-c9f3e1b064e8",
   "metadata": {},
   "source": [
    "### PyTorch: Tensors and autograd\n",
    "\n",
    "- **Problem:** Manually implementing the backward pass for calculating gradients is simple for small networks but becomes very difficult for large, complex ones.\n",
    "\n",
    "- **Solution:** PyTorch's `autograd` package automates the calculation of gradients.\n",
    "\n",
    "- **How it Works:**\n",
    "    - The forward pass defines a computational graph where Tensors are the nodes.\n",
    "    - `autograd` uses this graph to automatically compute gradients during backpropagation.\n",
    "\n",
    "- **Usage:**\n",
    "    - If a Tensor `x` has `x.requires_grad=True`, its gradient is stored in `x.grad`.\n",
    "\n",
    "- **Next Step:** Revisit the polynomial fitting example using autograd, which eliminates the need to manually code the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec41dc-db50-45e4-ad0a-e04980c54879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
    "# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c835b-3404-4765-b3a7-01089f1e5f26",
   "metadata": {},
   "source": [
    "### PyTorch: nn\n",
    "\n",
    "- **Problem:** Using raw `autograd` can be too low-level and cumbersome for building large neural networks.\n",
    "\n",
    "- **Common Abstraction:** Neural networks are usually constructed by arranging computations into layers, which contain learnable parameters.\n",
    "\n",
    "- **PyTorch's Solution:** The `nn` package provides a high-level abstraction for building networks, similar to Keras in TensorFlow.\n",
    "\n",
    "- **Key Features of `nn`:**\n",
    "    - It provides `Modules`, which act as neural network layers.\n",
    "    - A `Module` can hold state, including learnable parameters.\n",
    "    - The package also includes a library of common loss functions.\n",
    "\n",
    "- **Next Step:** Implement the polynomial model using the higher-level abstractions from the `nn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f486b4-212b-462e-8dff-257ce53767bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# For this example, the output y is a linear function of (x, x^2, x^3), so\n",
    "# we can consider it as a linear layer neural network. Let's prepare the\n",
    "# tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n",
    "# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n",
    "# of shape (2000, 3) \n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. The Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n",
    "# to match the shape of `y`.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# You can access the first layer of `model` like accessing the first item of a list\n",
    "linear_layer = model[0]\n",
    "\n",
    "# For linear layer, its parameters are stored as `weight` and `bias`.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + \\\n",
    "{linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a1872-ea46-4f5c-af28-f6f4995fd86e",
   "metadata": {},
   "source": [
    "### PyTorch: optim\n",
    "\n",
    "- **Problem:** Manually updating model weights is simple for basic stochastic gradient descent but is impractical for more advanced optimizers like Adam, AdaGrad, or RMSProp.\n",
    "\n",
    "- **Solution:** The `optim` package in PyTorch provides implementations of many common optimization algorithms.\n",
    "\n",
    "- **Function:** It abstracts the weight update process, allowing you to easily use sophisticated optimizers.\n",
    "\n",
    "- **Next Step:** We will now train the `nn` model using the `RMSprop` algorithm from the `optim` package instead of updating weights manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef8881-c838-47dd-936d-c69e3c46cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + \\\n",
    "{linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n",
    "\n",
    "# Plot prediction\n",
    "y_pred = linear_layer.bias.item() + linear_layer.weight[:, 0].item() * x + \\\n",
    "linear_layer.weight[:, 1].item() * x**2 + linear_layer.weight[:, 2].item() * x**3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, label=\"True model\")\n",
    "ax.plot(x, y_pred, label=\"Learned model\")\n",
    "ax.set(xlabel=\"x\", ylabel=r\"$y=\\sin(x)$\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c6eb1-9a04-4f2c-a90a-e995c813a332",
   "metadata": {},
   "source": [
    "### Exercises and further reading\n",
    "\n",
    "https://www.learnpytorch.io\n",
    "\n",
    "https://docs.pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b42245-6071-4209-9582-d2756b48fe48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
